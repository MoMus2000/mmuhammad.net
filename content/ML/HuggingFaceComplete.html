<!doctype html>
<html lang="en">
<head>
<style>
pre {
white-space: pre-line;
word-wrap: break-word;
}
</style>
<!-- Required meta tags -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<!-- Bootstrap CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

<title> {{.Topic}} </title>
</head>
<body>

<div class = "container-fluid">

<article>

<pre>
<style>
.gist {overflow:auto;}
.gist .blob-wrapper.data {max-height:350px;overflow:auto;}
.centered { margin: 0 auto; width: 50%; }
@media only screen and (min-device-width : 320px) and (max-device-width : 480px) 
{ .centered{margin: 0 auto; width: 100%;}}
</style>
</pre>

<div class="centered">

<pre><h2><strong>Generating shakesperean poetry using GPT-2 and HuggingFace</strong></h2></pre>
<pre><h3>
GPT-2 is an open-source transformer that is able to translate text, answer questions, summarize passages and generate text output. The model was createdby open-ai and is available to be used with the huggging face API.
</h3><h3>
HuggingFace is best described as a community that provides a library to interact with and use transformers for tasks related to nlp, cv and audio. It also has avast collection of datasets available to use as well.
</h3><h3>
In this tutorial we will explore how to use a collection of shakespeares sonnets to train on GPT-2 and have it output some sample poetry based on some provided seed text.
</h3><h3>
To follow along, a GPU is essential otherwise a google colab notebook instance may work fine.
</h3><h2>
<strong>Its also best to know what transfer learning is as well.</strong>
</h2><h3>
Copied from wikipedia: Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.
</h3><h3>
In short by using a langauge model like GPT-2 which is trained on raw text data, we can use the hugging face libary and fine tune GPT-2 to generate poetry.
</h3>
</pre>
<script src="https://gist.github.com/MoMus2000/23a0a4d2b24423e4b83bf06139ad781c.js"></script>
<pre>
Once we have our imports we can create our text based dataset and initialize our model and tokenizer
</pre>
<script src="https://gist.github.com/MoMus2000/a5dfe8dea5b1b527d5811fda2a77d4ef.js"></script>
<pre>Our dataset is a txt file that has all the sonnets of shakespeare pasted inside of it, no additional work required.</pre>
<pre>The next step is to load in the dataset, using the snippet below</pre>
<script src="https://gist.github.com/MoMus2000/4b87a2c47e1b6ba659b0cfab903ae47c.js"></script>
<pre>We then define our training arguments and our train function, provided by the hugging face API</pre>
<script src="https://gist.github.com/MoMus2000/1d32028ccdbb62ee7c8c4190f73ffb7e.js"></script>
<pre>Finally we can print out the output and reap the fruits of our labour!</pre>
<script src="https://gist.github.com/MoMus2000/fb09af70f6c45a91c51d5e4ccfb7c390.js"></script>
<pre>Final generated poetry:</pre>
<script src="https://gist.github.com/MoMus2000/494ed6e3a0bc3589dfb4a637c7d64efa.js"></script>
<pre><h2>In conclusion</h2></pre>
<pre><h3>We can see clearly from the output that model picked up on the style found in provided shakespeare sonnets. Some ways to improve output would be to:
</h3></pre>
<pre><h3><li>Specify start and end of a verse</li></h3></pre>
<pre><h3><li>Train for a longer period of time (mine was 15 minutes)</li></h3></pre>
<pre><h3><li>Play around with the training parameters to see what gives better results</li></h3></pre>
</div>
</article>

</div>


<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
</body>
</html>